{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to <img src=\"../logo/snnax.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "This is notebook contains a comprehensive introduction to `snnax`. This notebook will teach you how to train a simple spiking convolutional neural network on the DVS gestures dataset. It is not a comprehensive introduction into spiking neural networks itself and assumes that you know at least the basics of modeling them as discretized ODEs/RNNs. If you want to know more about spiking neural networks and how to train them, have a look at (Emre's and Jasons paper, Neural Dynamics Book).\n",
    "\n",
    "We start by importing some of the basic packages for JAX and other helper tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as nn\n",
    "import jax.random as jrand\n",
    "from jax.tree_util import tree_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import `snnax` and the underlying neural network package `equinox` as well as `optax` which provides optimizers like Adam and basic loss functions like cross-entropy and L2 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Platform 'METAL' is experimental and not all JAX functionality may be correctly supported!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M3 Max\n",
      "\n",
      "systemMemory: 64.00 GB\n",
      "maxCacheSize: 24.00 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1722257825.767133  331232 mps_client.cc:510] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!\n",
      "I0000 00:00:1722257825.777303  331232 service.cc:145] XLA service 0x13856ac80 initialized for platform METAL (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1722257825.777315  331232 service.cc:153]   StreamExecutor device (0): Metal, <undefined>\n",
      "I0000 00:00:1722257825.779812  331232 mps_client.cc:406] Using Simple allocator.\n",
      "I0000 00:00:1722257825.779831  331232 mps_client.cc:384] XLA backend will use up to 51537821696 bytes on device 0 for SimpleAllocator.\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "import snnax.snn as snn\n",
    "import equinox as eqx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we import the `tonic` package to get easy access to the DVS Gestures dataset. We also import the PyTorch dataloader since it ahs many desirable features such as options for multiple workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tonic.datasets import DVSGesture\n",
    "from tonic.transforms import Compose, Downsample, ToFrame\n",
    "from utils import calc_accuracy, DVSGestures, RandomSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the dataset. We are going to train a three-layer spiking CNN on the DVS Gestures dataset that can be found under [Paper](https://ieeexplore.ieee.org/document/8100264). Instead of downloading the dataset by hand and defining everything by ourselves, we use the `tonic` package to automate this. This package also contains a lot of useful transformations that help us the bring the data into the right shape.\n",
    "\n",
    "In particular, it contains the `Downsample` and `ToFrame` transformations which reduce the resolution and bin all the events of shape (polarity, timestamp, x-position, y-position) into a voxel representation so that is can be efficiently processed using our SNN.\n",
    "\n",
    "We also define some of the usual hyperparameters here for later use. You can modify them according to your hardware.\n",
    "\n",
    "⚠️ **Warning!** The download might take a while, depending on your connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/38022171/ibmGestureTrain.tar.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20240729/eu-west-1/s3/aws4_request&X-Amz-Date=20240729T125711Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=6ab1daa59777d38297245c02fc1ee98707f0a8f3daa7aef1f71b0f7c5325ac58 to ./data/DVSGesture/ibmGestureTrain.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b6d166d6d5408b9670916010fc19f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2443675558 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/DVSGesture/ibmGestureTrain.tar.gz to ./data/DVSGesture\n",
      "Downloading https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/38020584/ibmGestureTest.tar.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20240729/eu-west-1/s3/aws4_request&X-Amz-Date=20240729T153405Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=a528fa64b26fdb1d6eb4f42f8a1d692a63b1a9432ba05311610a2d834e2b4476 to ./data/DVSGesture/ibmGestureTest.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5a2e0d3dbc48808abd407712ef5847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/691455012 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/DVSGesture/ibmGestureTest.tar.gz to ./data/DVSGesture\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCHSIZE = 32\n",
    "TIMESTEPS = 500 # Number of bins/time slices in our voxel grid\n",
    "TIMESTEPS_TEST = 1798 # the smallest sequence length in the test set\n",
    "SCALING = .25 # How much we downscale the initial resolution of 128x128\n",
    "SENSOR_WIDTH = int(128*SCALING)\n",
    "SENSOR_HEIGHT = int(128*SCALING)\n",
    "SENSOR_SIZE = (2, SENSOR_WIDTH, SENSOR_HEIGHT) # Input shape of a single time slice\n",
    "SEED = 42 # Random seed\n",
    "\n",
    "# Downsample and ToFrames have to be applied last if we want to do other transformation too!\n",
    "# Initial dataset size is 128x128\n",
    "train_transform = Compose([Downsample(time_factor=1., \n",
    "                                        spatial_factor=SCALING),\n",
    "                            ToFrame(sensor_size=(SENSOR_HEIGHT, SENSOR_WIDTH, 2), \n",
    "                                    n_time_bins=TIMESTEPS)])\n",
    "\n",
    "trainset = DVSGesture(save_to=\"./data\", train=True, transform=train_transform)\n",
    "testset = DVSGesture(save_to=\"./data\", train=False, transform=train_transform)\n",
    "train_dataset = DVSGestures(\"data/DVSGesture/ibmGestureTrain\", \n",
    "                            sample_duration=TIMESTEPS,\n",
    "                            transform=train_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCHSIZE, num_workers=4)\n",
    "\n",
    "# Test data loading\n",
    "test_transform = Compose([RandomSlice(TIMESTEPS_TEST, seed=SEED),\n",
    "                        Downsample(time_factor=1., \n",
    "                                    spatial_factor=SCALING),\n",
    "                        ToFrame(sensor_size=(SENSOR_HEIGHT, SENSOR_WIDTH, 2), \n",
    "                                n_time_bins=TIMESTEPS_TEST)])\n",
    "\n",
    "test_dataset = DVSGestures(\"data/DVSGesture/ibmGestureTest\", \n",
    "                            transform=test_transform)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=BATCHSIZE, num_workers=4)\n",
    "\n",
    "# Labels for the prediction and reference\n",
    "NUM_LABELS = 11\n",
    "LABELS = [\"hand clap\",\n",
    "        \"right hand wave\",\n",
    "        \"left hand wave\",\n",
    "        \"right arm clockwise\",\n",
    "        \"right arm counterclockwise\",\n",
    "        \"left arm clockwise\",\n",
    "        \"left arm counterclockwise\",\n",
    "        \"arm roll\",\n",
    "        \"air drums\",\n",
    "        \"air guitar\",\n",
    "        \"other gestures\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we proceed to define the model. Since `snnax` is build on `equinox` which exposes a PyTorch-like API for defining neural networks, we can quickly and elegantly define our spiking CNN.\n",
    "\n",
    "We want to build a simple feed-forward network for which we can use the `snnax.Sequential` class which consecutively executes the given layers. It also takes care of the state management of the membrane potentials of the spiking neuron layersusing a `jax.lax.scan` primitive.\n",
    "We define 3 layers of convolutions with a kernel size of 7. The first layer has stride two and 32 output channels while the other two have a stride of 1 and 64 output channels. We do not use a bias as is common in many SNN architectures.\n",
    "This can be easily done by just interleaving the `equinox.nn.Conv2d` layers with `snnax.LIF` layers and passing them the appropriate parameters.\n",
    "Notice that since `snnax` is build on `equinox`, you can use all layer types defined there in snnax as well. \n",
    "The output of the third layer is flattened and fed into a linear layer which has 11 output neurons for the 11 classes.\n",
    "We also add some dropout to help with overfitting.\n",
    "\n",
    "❗️**Important** There is one peculiar thing about defining layers in equinox that seems to be very annoying in the beginning, but is actually very useful for serious science and reproducibility: Every layer has the keyword argument `key` which takes a `jax.random.PRNGKey` as input. This argument is an artifact of the implementation of random numbers in `JAX`. All random numbers in `JAX` are initialized using a Pseudo-Random-Number-Generator-Key or short `PRNGKey` so that we have maximum control over the randomness in our initializations of the network weights, biases and membrane potentials. Using the same key over and over again will always lead to the same outcome, so make sure that for every layer you create enough keys using `jax.random.split` and distribute them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jrand.PRNGKey(SEED)\n",
    "init_key, key = jrand.split(key, 2)\n",
    "keys = jrand.split(init_key, 4)\n",
    "\n",
    "model = snn.Sequential(\n",
    "    eqx.nn.Conv2d(2, 32, 7, 2, key=keys[0], use_bias=False),\n",
    "    snn.LIF([.95, .85]),\n",
    "    eqx.nn.Dropout(p=.25),\n",
    "\n",
    "    eqx.nn.Conv2d(32, 64, 7, 1, key=keys[1], use_bias=False),\n",
    "    snn.LIF([.95, .85]),\n",
    "    eqx.nn.Dropout(p=.25),\n",
    "\n",
    "    eqx.nn.Conv2d(64, 64, 7, 1, key=keys[2], use_bias=False),\n",
    "    snn.LIF([.95, .85]),\n",
    "    eqx.nn.Dropout(p=.25),\n",
    "\n",
    "    snn.Flatten(),\n",
    "    eqx.nn.Linear(64, 11, key=keys[3], use_bias=False),\n",
    "    snn.LIF([.95, .9])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We move on to define the loss function of our model. This is particularly easy and one of the many instances where `JAX` really shines.\n",
    "As opposed to other frameworks, we can define our loss function for a single sample only and then use the `jax.vmap` function transformation to automatically batchify this function. Use the `None` keyword for the arguments of your function that you do not want to batchify. Learn more about the awesome features of `JAX` under [JAX Introduction](https://jax.readthedocs.io/en/latest/quickstart.html#auto-vectorization-with-jax-vmap).\n",
    "\n",
    "It is time to have a quick talk about the intricacies of `JAX` and `equinox` when it comes to stateful computations and the management of parameters.\n",
    "As you may know, `JAX` leverages a functional programming paradigm, which roughly means that functions have to be pure and not have any side-effects on variables that are not in the input arguments and output values.\n",
    "This paradigm enables to express a lot of the cool features of `JAX` as function transformations, meaning that we define a function and then decorate it with the appropriate decorator, e.g. `@jax.vmap`. Other examples are `@jax.grad` and `@jax.jit`.\n",
    "\n",
    "However, when it comes to neural networks, this can have several disadvantages. A neural network has possibly hundered of throusands of parameters and including them all explicitly in the arguments of a function would be cumbersome.\n",
    "Entry `equinox` and `PyTrees`. A PyTree is a data structure that allows to store many parameters in a hierarchical manner so that those that belong to the same layer or module are stored together. However, the parameters alone do not make up the model. Thus, `equinox` defines a `equinox.Module` class that is essentially an executable PyTree. This is fantastic because now we can just feed this object (what we called `model` in this tutorial) to all our functions and have the neural network function and its parameters in one place.\n",
    "Instead of hundereds of arguments that we need to feed to our loss function, we now have a single one.\n",
    "Internally, `equinox` flattens the PyTree into a list and puts every parameter to its appropriate place. The neural network has to be pure function after all, but this small detail is hidden from the user.\n",
    "\n",
    "However, there is a slight problem with this approach: Some parameters in the PyTree such as activation functions or integer values are parameters that we want to be ignored for certain function transformations, e.g. automatic differentiation.\n",
    "`equinox` provides a filtering function for this called `equinox.filter`, that allows you to filter the PyTree for certain parameter types such as floating point arrays. \n",
    "There are several convenience wrappers around the major function transformations such as `equinox.filter_jit`, `equinox.filter_grad` etc. that take care of this. They assume that the model is contained in the PyTree that is the **first** argument of the function we want to transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.vmap, in_axes=(None, None, 0, 0, 0))\n",
    "def loss_fn(model, init_states, data, target, key):\n",
    "    # Loss function for a single example\n",
    "    states, outs = model(init_states, data, key=key)\n",
    "\n",
    "    # Get the output of last layer\n",
    "    final_layer_out = outs[-1]\n",
    "\n",
    "    # Sum all spikes in each output neuron along time axis\n",
    "    pred = tree_map(lambda x: jnp.sum(x, axis=0), final_layer_out)\n",
    "    \n",
    "    # We use cross-entropy since we have a classification task\n",
    "    return optax.softmax_cross_entropy(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the gradient with respect to the loss function is now just the application of another function transformation, i.e. `equinox.filter_value_and_grad` which makes our function return a tuple where the first output is the loss and the second output is a PyTree that is of the same shape as the model's PyTree but instead contains the parameters gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_value_and_grad\n",
    "def loss_and_grads(model, init_states, data, target, key):\n",
    "    keys = jrand.split(key, BATCHSIZE)\n",
    "    return jnp.sum(loss_fn(model, init_states, data, target, keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the update function that uses the gradients to update the model parameters and optimizer state. Due to the functional programming approach, we have to explicitly take care of the optimizer state, which is just another clone of the models PyTree with the optimizer's parameter updates as leaves.\n",
    "\n",
    "The `equinox.apply_updates` function applies these updates to the models parameters.\n",
    "\n",
    "Also we use the `equinox.filter_jit` which is just a simple wrapper around `jax.jit` to just-in-time compile our entire training workflow and make it much faster.\n",
    "\n",
    "❗️**Important**: Before we can use the model, we first have to initialize the models initial states, i.e. membrane potentials using `model.init_state` which then traverses the model and outputs a PyTree that contains the initial states of the stateful layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def update(model, optim, opt_state, data, targets, key):\n",
    "    init_key, grad_key = jrand.split(key)\n",
    "    # Initialize the states of the model.\n",
    "    states = model.init_state(SENSOR_SIZE, init_key)\n",
    "    loss_value, grads = loss_and_grads(model, states, data, targets, grad_key)    \n",
    "\n",
    "    # Update the models parameters with the updates from the optimizer\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the training loop. We us `optax` to create a Adam optimizer and create the optimizer state by filtering the model PyTree for all floating-point arrays using the aforementioned `equinox.filter` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:43<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "vmap got inconsistent sizes for array axes to be mapped:\n  * most axes (2 of them) had size 32, e.g. axis 0 of argument data of type float32[32,500,2,32,32];\n  * one axis had size 2: axis 0 of argument key of type uint32[2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m one_hot_target_batch \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mone_hot(target_batch, NUM_LABELS)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Use the update function to update the model and optimizer state for every step\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m model, opt_state, loss \u001b[38;5;241m=\u001b[39m \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_target_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m/\u001b[39mBATCHSIZE)\n\u001b[1;32m     24\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m/\u001b[39mBATCHSIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m, in \u001b[0;36mupdate\u001b[0;34m(model, optim, opt_state, data, targets, key)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the states of the model.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minit_state(SENSOR_SIZE, init_key)\n\u001b[0;32m----> 6\u001b[0m loss_value, grads \u001b[38;5;241m=\u001b[39m \u001b[43mloss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_key\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Update the models parameters with the updates from the optimizer\u001b[39;00m\n\u001b[1;32m      9\u001b[0m updates, opt_state \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mupdate(grads, opt_state)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m, in \u001b[0;36mloss_and_grads\u001b[0;34m(model, init_states, data, target, key)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@eqx\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_value_and_grad\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_and_grads\u001b[39m(model, init_states, data, target, key):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39msum(\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/envs/snnax/lib/python3.12/site-packages/jax/_src/api.py:1322\u001b[0m, in \u001b[0;36m_mapped_axis_size\u001b[0;34m(fn, tree, vals, dims, name)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1321\u001b[0m     msg\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  * some axes (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mct\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of them) had size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msz\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, e.g. axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00max\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1322\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(msg)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: vmap got inconsistent sizes for array axes to be mapped:\n  * most axes (2 of them) had size 32, e.g. axis 0 of argument data of type float32[32,500,2,32,32];\n  * one axis had size 2: axis 0 of argument key of type uint32[2]"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "optim = optax.adam(LR)\n",
    "opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "ebar = tqdm(range(EPOCHS))\n",
    "\n",
    "for epoch in ebar:\n",
    "    losses = []\n",
    "    \n",
    "    pbar = tqdm(train_dataloader, leave=False)\n",
    "    for input_batch, target_batch in pbar:\n",
    "        model_key, batch_key, key = jrand.split(key, 3)\n",
    "\n",
    "        # Convert the input and target to JAX arrays\n",
    "        input_batch = jnp.asarray(input_batch.numpy(), dtype=jnp.float32)\n",
    "        target_batch = jnp.asarray(target_batch.numpy(), dtype=jnp.float32)\n",
    "\n",
    "        # Make the target labels one-hot encoded\n",
    "        one_hot_target_batch = nn.one_hot(target_batch, NUM_LABELS)\n",
    "\n",
    "        # Use the update function to update the model and optimizer state for every step\n",
    "        model, opt_state, loss = update(model, optim, opt_state, input_batch, one_hot_target_batch, model_key)\n",
    "            \n",
    "        losses.append(loss/BATCHSIZE)\n",
    "        pbar.set_description(f\"loss: {loss/BATCHSIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test on the test dataset and check how well our model did. Note that this tutorial is not optimized for maximum performance on the dataset and there are surely better ways to achieve SOTA benchmarks. Feel free to improve on it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbar = tqdm(test_dataloader)  \n",
    "test_accuracies = []\n",
    "\n",
    "# This simple line disables the randomness introduced by the dropout layers\n",
    "model = eqx.tree_inference(model, True)\n",
    "\n",
    "for input_batch, target_batch in tbar:\n",
    "    batch_key, key = jrand.split(key, 2)\n",
    "    input_batch = jnp.asarray(input_batch.numpy(), dtype=jnp.float32)\n",
    "    target_batch = jnp.asarray(target_batch.numpy(), dtype=jnp.float32)\n",
    "\n",
    "    init_states = model.init_state(SENSOR_SIZE, batch_key)\n",
    "    test_acc = calc_accuracy(model, init_states, input_batch, target_batch, key)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "print(f\"test_accuracy = {jnp.mean(test_accuracies):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snnax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
