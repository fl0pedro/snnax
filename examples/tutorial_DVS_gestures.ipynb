{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to SNNAX\n",
    "\n",
    "This is notebook contains a comprehensive introduction to `snnax`. This notebook will teach you how to train a simple spiking convolutional neural network on the DVS gestures dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as nn\n",
    "import jax.random as jrand\n",
    "from jax.tree_util import tree_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import snnax and the underlying neural network package equinox as well as optax which provides optimizers like ADAM and basic loss functions like cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import snnax.snn as snn\n",
    "import equinox as eqx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we import the tonic package to get easy access to the DVS Gestures dataset. We also import the PyTorch dataloader since it ahs many desirable features such as options for multiple workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tonic.transforms import Compose, Downsample, ToFrame\n",
    "from utils import calc_accuracy, DVSGestures, RandomSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the dataloading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_tonic/DVSGesture/ibmGestureTrain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Downsample and ToFrames have to be applied last!\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Initial dataset size is 128x128\u001b[39;00m\n\u001b[1;32m     13\u001b[0m train_transform \u001b[38;5;241m=\u001b[39m Compose([Downsample(time_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m, \n\u001b[1;32m     14\u001b[0m                                         spatial_factor\u001b[38;5;241m=\u001b[39mSCALING),\n\u001b[1;32m     15\u001b[0m                             ToFrame(sensor_size\u001b[38;5;241m=\u001b[39m(SENSOR_HEIGHT, SENSOR_WIDTH, \u001b[38;5;241m2\u001b[39m), \n\u001b[1;32m     16\u001b[0m                                     n_time_bins\u001b[38;5;241m=\u001b[39mTIMESTEPS)])\n\u001b[0;32m---> 18\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDVSGestures\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_tonic/DVSGesture/ibmGestureTrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                            \u001b[49m\u001b[43msample_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, \n\u001b[1;32m     23\u001b[0m                             shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     24\u001b[0m                             batch_size\u001b[38;5;241m=\u001b[39mBATCHSIZE, \n\u001b[1;32m     25\u001b[0m                             num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Test data loading                             \u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/snnax/examples/utils.py:49\u001b[0m, in \u001b[0;36mDVSGestures.__init__\u001b[0;34m(self, path, transform, target_transform, sample_duration)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_stamps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39muint16), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39muint16), \\\n\u001b[1;32m     47\u001b[0m                         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu1\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39muint32)])\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m usrdir \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     50\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, os\u001b[38;5;241m.\u001b[39mfsdecode(usrdir))\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(path):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_tonic/DVSGesture/ibmGestureTrain'"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCHSIZE = 32\n",
    "TIMESTEPS = 500\n",
    "TIMESTEPS_TEST = 1798 # the smallest sequence length in the test set\n",
    "SENSOR_WIDTH = 32\n",
    "SENSOR_HEIGHT = 32\n",
    "SCALING = .25 # .5\n",
    "SENSOR_SIZE = (2, SENSOR_WIDTH, SENSOR_HEIGHT)\n",
    "SEED = 42\n",
    "\n",
    "# Downsample and ToFrames have to be applied last!\n",
    "# Initial dataset size is 128x128\n",
    "train_transform = Compose([Downsample(time_factor=1., \n",
    "                                        spatial_factor=SCALING),\n",
    "                            ToFrame(sensor_size=(SENSOR_HEIGHT, SENSOR_WIDTH, 2), \n",
    "                                    n_time_bins=TIMESTEPS)])\n",
    "\n",
    "train_dataset = DVSGestures(\"data_tonic/DVSGesture/ibmGestureTrain\", \n",
    "                            sample_duration=TIMESTEPS,\n",
    "                            transform=train_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                            shuffle=True, \n",
    "                            batch_size=BATCHSIZE, \n",
    "                            num_workers=8)\n",
    "\n",
    "# Test data loading                             \n",
    "test_transform = Compose([RandomSlice(TIMESTEPS_TEST, seed=SEED),\n",
    "                        Downsample(time_factor=1., \n",
    "                                    spatial_factor=SCALING),\n",
    "                        ToFrame(sensor_size=(SENSOR_HEIGHT, SENSOR_WIDTH, 2), \n",
    "                                n_time_bins=TIMESTEPS_TEST)])\n",
    "\n",
    "test_dataset = DVSGestures(\"data_tonic/DVSGesture/ibmGestureTest\", \n",
    "                            transform=test_transform)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                            shuffle=True, \n",
    "                            batch_size=BATCHSIZE, \n",
    "                            num_workers=8)\n",
    "\n",
    "# Labels for the prediction\n",
    "NUM_LABELS = 11\n",
    "LABELS = [\"hand clap\",\n",
    "        \"right hand wave\",\n",
    "        \"left hand wave\",\n",
    "        \"right arm clockwise\",\n",
    "        \"right arm counterclockwise\",\n",
    "        \"left arm clockwise\",\n",
    "        \"left arm counterclockwise\",\n",
    "        \"arm roll\",\n",
    "        \"air drums\",\n",
    "        \"air guitar\",\n",
    "        \"other gestures\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we proceed to define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m init_key, key \u001b[38;5;241m=\u001b[39m jrand\u001b[38;5;241m.\u001b[39msplit(key, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m keys \u001b[38;5;241m=\u001b[39m jrand\u001b[38;5;241m.\u001b[39msplit(key, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43msnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLIF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.85\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLIF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.85\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLIF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.85\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFlatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLIF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/snnax/lib/python3.12/site-packages/equinox/_module.py:548\u001b[0m, in \u001b[0;36m_ModuleMeta.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m initable_cls \u001b[38;5;241m=\u001b[39m _make_initable(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m, post_init, wraps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# [Step 2] Instantiate the class as normal.\u001b[39;00m\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_ModuleMeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitable_cls\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_abstract(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# [Step 3] Check that all fields are occupied.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Projects/snnax/src/snnax/snn/composed.py:23\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[0;34m(self, forward_fn, *layers)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m     20\u001b[0m             \u001b[38;5;241m*\u001b[39mlayers: Sequence[eqx\u001b[38;5;241m.\u001b[39mModule],\n\u001b[1;32m     21\u001b[0m             forward_fn: Callable \u001b[38;5;241m=\u001b[39m default_forward_fn) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(layers))\n\u001b[0;32m---> 23\u001b[0m     input_connectivity, input_layer_ids, final_layer_ids \u001b[38;5;241m=\u001b[39m gen_feed_forward_struct(num_layers)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Constructing the connectivity graph\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     graph_structure \u001b[38;5;241m=\u001b[39m GraphStructure(\n\u001b[1;32m     27\u001b[0m         num_layers \u001b[38;5;241m=\u001b[39m num_layers,\n\u001b[1;32m     28\u001b[0m         input_layer_ids \u001b[38;5;241m=\u001b[39m input_layer_ids,\n\u001b[1;32m     29\u001b[0m         final_layer_ids \u001b[38;5;241m=\u001b[39m final_layer_ids,\n\u001b[1;32m     30\u001b[0m         input_connectivity \u001b[38;5;241m=\u001b[39m input_connectivity\n\u001b[1;32m     31\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "key = jrand.PRNGKey(SEED)\n",
    "init_key, key = jrand.split(key, 2)\n",
    "keys = jrand.split(key, 4)\n",
    "\n",
    "model = snn.Sequential(\n",
    "    eqx.nn.Conv2d(2, 32, 7, 2, key=keys[0], use_bias=False),\n",
    "    snn.LIF([.95, .85]),\n",
    "    eqx.nn.Dropout(p=.25),\n",
    "\n",
    "    eqx.nn.Conv2d(32, 64, 7, 1, key=keys[1], use_bias=False),\n",
    "    snn.LIF([.95, .85]),\n",
    "    eqx.nn.Dropout(p=.25),\n",
    "\n",
    "    eqx.nn.Conv2d(64, 64, 7, 1, key=keys[2], use_bias=False),\n",
    "    snn.LIF([.95, .85]),\n",
    "    eqx.nn.Dropout(p=.25),\n",
    "\n",
    "    snn.Flatten(),\n",
    "    eqx.nn.Linear(64, 11, key=keys[3], use_bias=False),\n",
    "    snn.LIF([.95, .9])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define how the loss is exacly calculated, i.e. whether\n",
    "we use a sum of spikes or spike-timing for the calculation of\n",
    "the cross-entropy. For a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.vmap(in_axes=(None, None, 0, 0, 0))\n",
    "def loss_fn(model, init_states, data, target, key):\n",
    "    states, outs = model(init_states, data, key=key)\n",
    "\n",
    "    # Get the output of last layer\n",
    "    final_layer_out = outs[-1]\n",
    "    # Sum over all spikes in each output neuron along time axis\n",
    "    pred = tree_map(lambda x: jnp.sum(x, axis=0), final_layer_out)\n",
    "    return optax.softmax_cross_entropy(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the function to compute the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_value_and_grad\n",
    "def loss_and_grads(model, init_states, data, target, key):\n",
    "    return jnp.sum(loss_fn(model, init_states, data, target, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the update function\n",
    "Function to calculate the update of the model and the optimizer based\n",
    "on the calculated updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def update(model,\n",
    "            optim, \n",
    "            opt_state, \n",
    "            input_batch, \n",
    "            target_batch, \n",
    "            loss_fn, \n",
    "            key):\n",
    "    \"\"\"\n",
    "    Function to calculate the update of the model and the optimizer based\n",
    "    on the calculated updates.\n",
    "    \"\"\"\n",
    "    init_key, grad_key = jax.random.split(key)\n",
    "    states = model.init_state(SENSOR_SIZE, init_key)\n",
    "    loss_value, grads = loss_and_grads(model, \n",
    "                                        states, \n",
    "                                        input_batch, \n",
    "                                        target_batch, \n",
    "                                        loss_fn, \n",
    "                                        grad_key)    \n",
    "\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "\n",
    "optim = optax.adam(LR)\n",
    "opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "nbar = tqdm(range(EPOCHS))\n",
    "\n",
    "for epoch in nbar:\n",
    "    losses = []\n",
    "    \n",
    "    pbar = tqdm(train_dataloader, leave=False)\n",
    "    for input_batch, target_batch in pbar:\n",
    "        model_key, batch_key, key = jrand.split(key, 3)\n",
    "        input_batch = jnp.asarray(input_batch.numpy(), dtype=jnp.float32)\n",
    "        target_batch = jnp.asarray(target_batch.numpy(), dtype=jnp.float32)\n",
    "        one_hot_target_batch = jnp.asarray(nn.one_hot(target_batch, NUM_LABELS), \n",
    "                                            dtype=jnp.float32)\n",
    "\n",
    "        model, opt_state, loss = update(model, \n",
    "                                        optim,\n",
    "                                        opt_state,  \n",
    "                                        input_batch,\n",
    "                                        one_hot_target_batch,\n",
    "                                        model_key\n",
    "                                    )\n",
    "            \n",
    "        losses.append(loss/BATCHSIZE)\n",
    "        \n",
    "        pbar.set_description(f\"loss: {loss/BATCHSIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbar = tqdm(test_dataloader, leave=False)  \n",
    "test_accuracies = []\n",
    "for input_test, target_test in tbar:\n",
    "    batch_key, key = jrand.split(key, 2)\n",
    "    input_batch = jnp.asarray(input_test.numpy(), dtype=jnp.float32)\n",
    "    target_batch = jnp.asarray(target_test.numpy(), dtype=jnp.float32)\n",
    "    test_acc = calc_accuracy(model, \n",
    "                            model.init_state(SENSOR_SIZE, batch_key), \n",
    "                            input_batch, \n",
    "                            target_batch,\n",
    "                            key)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "model = eqx.tree_inference(model, False)\n",
    "\n",
    "nbar.set_description(f\"epoch: {epoch}, \"\n",
    "                    f\"loss = {jnp.mean(losses)}, \"\n",
    "                    f\"test_accuracy = {jnp.mean(test_accuracies):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snnax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
