{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to SNNAX\n",
    "\n",
    "This is notebook contains a comprehensive introduction to `snnax`. This notebook will teach you how to train a simple spiking convolutional neural network on the DVS gestures dataset. It is not a comprehensive introduction into spiking neural networks itself and assumes that you know at least the basics of modeling them as discretized ODEs/RNNs. If you want to know more about spiking neural networks and how to train them, have a look at (Emre's and Jasons paper, Neural Dynamics Book).\n",
    "\n",
    "We start by importing some of the basic packages for JAX and other helper tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as nn\n",
    "import jax.random as jrand\n",
    "from jax.tree_util import tree_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import `snnax` and the underlying neural network package `equinox` as well as `optax` which provides optimizers like Adam and basic loss functions like cross-entropy and L2 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Platform 'METAL' is experimental and not all JAX functionality may be correctly supported!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M3 Max\n",
      "\n",
      "systemMemory: 64.00 GB\n",
      "maxCacheSize: 24.00 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1722219641.232486  120275 mps_client.cc:510] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!\n",
      "I0000 00:00:1722219641.243220  120275 service.cc:145] XLA service 0x34a692d70 initialized for platform METAL (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1722219641.243230  120275 service.cc:153]   StreamExecutor device (0): Metal, <undefined>\n",
      "I0000 00:00:1722219641.245594  120275 mps_client.cc:406] Using Simple allocator.\n",
      "I0000 00:00:1722219641.245610  120275 mps_client.cc:384] XLA backend will use up to 51537821696 bytes on device 0 for SimpleAllocator.\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "import snnax.snn as snn\n",
    "import equinox as eqx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we import the `tonic` package to get easy access to the DVS Gestures dataset. We also import the PyTorch dataloader since it ahs many desirable features such as options for multiple workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import tonic\n",
    "from tonic.transforms import Compose, Downsample, ToFrame\n",
    "from utils import calc_accuracy, DVSGestures, RandomSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the dataset. We are going to train a three-layer spiking CNN on the DVS Gestures dataset that can be found under [Paper](https://ieeexplore.ieee.org/document/8100264). Instead of downloading the dataset by hand and defining everything by ourselves, we use the `tonic` package to automate this. This package also contains a lot of useful transformations that help us the bring the data into the right shape.\n",
    "\n",
    "In particular, it contains the `Downsample` and `ToFrame` transformations which reduce the resolution and bin all the events of shape (polarity, timestamp, x-position, y-position) into a voxel representation so that is can be efficiently processed using our SNN.\n",
    "\n",
    "We also define some of the usual hyperparameters here for later use. You can modify them according to your hardware.\n",
    "\n",
    "**Warning!** The download might take a while, depending on your connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/38022171/ibmGestureTrain.tar.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20240729/eu-west-1/s3/aws4_request&X-Amz-Date=20240729T022045Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=31ec6777170f0eb597ca5ed361a139222e451e5e253913b80ddf257aad464b4e to ./data/DVSGesture/ibmGestureTrain.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78cc87da32ce4a0f9f71d325f6d13cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2443675558 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCHSIZE = 32\n",
    "TIMESTEPS = 500 # Number of bins/time slices in our voxel grid\n",
    "TIMESTEPS_TEST = 1798 # the smallest sequence length in the test set\n",
    "SCALING = .25 # How much we downscale the initial resolution of 128x128\n",
    "SENSOR_WIDTH = int(128*SCALING)\n",
    "SENSOR_HEIGHT = int(128*SCALING)\n",
    "SENSOR_SIZE = (2, SENSOR_WIDTH, SENSOR_HEIGHT) # Input shape of a single time slice\n",
    "SEED = 42 # Random seed\n",
    "\n",
    "# Downsample and ToFrames have to be applied last if we want to do other transformation too!\n",
    "# Initial dataset size is 128x128\n",
    "train_transform = Compose([Downsample(time_factor=1., \n",
    "                                        spatial_factor=SCALING),\n",
    "                            ToFrame(sensor_size=(SENSOR_HEIGHT, SENSOR_WIDTH, 2), \n",
    "                                    n_time_bins=TIMESTEPS)])\n",
    "testset = tonic.datasets.DVSGesture(save_to=\"./data\", train=True, transform=train_transform)\n",
    "train_dataset = DVSGestures(\"data/DVSGesture/ibmGestureTrain\", \n",
    "                            sample_duration=TIMESTEPS,\n",
    "                            transform=train_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                            shuffle=True, \n",
    "                            batch_size=BATCHSIZE, \n",
    "                            num_workers=8)\n",
    "\n",
    "# Test data loading                             \n",
    "test_transform = Compose([RandomSlice(TIMESTEPS_TEST, seed=SEED),\n",
    "                        Downsample(time_factor=1., \n",
    "                                    spatial_factor=SCALING),\n",
    "                        ToFrame(sensor_size=(SENSOR_HEIGHT, SENSOR_WIDTH, 2), \n",
    "                                n_time_bins=TIMESTEPS_TEST)])\n",
    "\n",
    "test_dataset = DVSGestures(\"data/DVSGesture/ibmGestureTest\", \n",
    "                            transform=test_transform)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                            shuffle=True, \n",
    "                            batch_size=BATCHSIZE, \n",
    "                            num_workers=8)\n",
    "\n",
    "# Labels for the prediction and reference\n",
    "NUM_LABELS = 11\n",
    "LABELS = [\"hand clap\",\n",
    "        \"right hand wave\",\n",
    "        \"left hand wave\",\n",
    "        \"right arm clockwise\",\n",
    "        \"right arm counterclockwise\",\n",
    "        \"left arm clockwise\",\n",
    "        \"left arm counterclockwise\",\n",
    "        \"arm roll\",\n",
    "        \"air drums\",\n",
    "        \"air guitar\",\n",
    "        \"other gestures\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we proceed to define the model. Since `snnax` is build on `equinox` which exposes a PyTorch-like API for defining neural networks, we can quickly and elegantly define our spiking CNN.\n",
    "\n",
    "We want to build a simple feed-forward network for which we can use the `snnax.Sequential` class which consecutively executes the given layers. It also takes care of the state management of the membrane potentials of the spiking neuron layersusing a `jax.lax.scan` primitive.\n",
    "We define 3 layers of convolutions with a kernel size of 7. The first layer has stride two and 32 output channels while the other two have a stride of 1 and 64 output channels. We do not use a bias as is common in many SNN architectures.\n",
    "This can be easily done by just interleaving the `equinox.nn.Conv2d` layers with `snnax.LIF` layers and passing them the appropriate parameters.\n",
    "Notice that since `snnax` is build on `equinox`, you can use all layer types defined there in snnax as well. \n",
    "The output of the third layer is flattened and fed into a linear layer which has 11 output neurons for the 11 classes.\n",
    "We also add some dropout to help with overfitting.\n",
    "\n",
    "**Important!** There is one peculiar thing about defining layers in equinox that seems to be very annoying in the beginning, but is actually very useful for serious science and reproducibility: Every layer has the keyword argument `key` which takes a `jax.random.PRNGKey` as input. This argument is an artifact of the implementation of random numbers in `JAX`. All random numbers in `JAX` are initialized using a Pseudo-Random-Number-Generator-Key or short `PRNGKey` so that we have maximum control over the randomness in our initializations of the network weights, biases and membrane potentials. Using the same key over and over again will always lead to the same outcome, so make sure that for every layer you create enough keys using `jax.random.split` and distribute them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m init_key, key \u001b[38;5;241m=\u001b[39m jrand\u001b[38;5;241m.\u001b[39msplit(key, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m keys \u001b[38;5;241m=\u001b[39m jrand\u001b[38;5;241m.\u001b[39msplit(key, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43msnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLIF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.85\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLIF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.85\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLIF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.85\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFlatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLIF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/snnax/lib/python3.12/site-packages/equinox/_module.py:548\u001b[0m, in \u001b[0;36m_ModuleMeta.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m initable_cls \u001b[38;5;241m=\u001b[39m _make_initable(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m, post_init, wraps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# [Step 2] Instantiate the class as normal.\u001b[39;00m\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_ModuleMeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitable_cls\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_abstract(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# [Step 3] Check that all fields are occupied.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Projects/snnax/src/snnax/snn/composed.py:23\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[0;34m(self, forward_fn, *layers)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m     20\u001b[0m             \u001b[38;5;241m*\u001b[39mlayers: Sequence[eqx\u001b[38;5;241m.\u001b[39mModule],\n\u001b[1;32m     21\u001b[0m             forward_fn: Callable \u001b[38;5;241m=\u001b[39m default_forward_fn) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(layers))\n\u001b[0;32m---> 23\u001b[0m     input_connectivity, input_layer_ids, final_layer_ids \u001b[38;5;241m=\u001b[39m gen_feed_forward_struct(num_layers)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Constructing the connectivity graph\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     graph_structure \u001b[38;5;241m=\u001b[39m GraphStructure(\n\u001b[1;32m     27\u001b[0m         num_layers \u001b[38;5;241m=\u001b[39m num_layers,\n\u001b[1;32m     28\u001b[0m         input_layer_ids \u001b[38;5;241m=\u001b[39m input_layer_ids,\n\u001b[1;32m     29\u001b[0m         final_layer_ids \u001b[38;5;241m=\u001b[39m final_layer_ids,\n\u001b[1;32m     30\u001b[0m         input_connectivity \u001b[38;5;241m=\u001b[39m input_connectivity\n\u001b[1;32m     31\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "key = jrand.PRNGKey(SEED)\n",
    "init_key, key = jrand.split(key, 2)\n",
    "keys = jrand.split(key, 4)\n",
    "\n",
    "model = snn.Sequential(\n",
    "    eqx.nn.Conv2d(2, 32, 7, 2, key=keys[0], use_bias=False),\n",
    "    snn.LIF([.95, .85]),\n",
    "    eqx.nn.Dropout(p=.25),\n",
    "\n",
    "    eqx.nn.Conv2d(32, 64, 7, 1, key=keys[1], use_bias=False),\n",
    "    snn.LIF([.95, .85]),\n",
    "    eqx.nn.Dropout(p=.25),\n",
    "\n",
    "    eqx.nn.Conv2d(64, 64, 7, 1, key=keys[2], use_bias=False),\n",
    "    snn.LIF([.95, .85]),\n",
    "    eqx.nn.Dropout(p=.25),\n",
    "\n",
    "    snn.Flatten(),\n",
    "    eqx.nn.Linear(64, 11, key=keys[3], use_bias=False),\n",
    "    snn.LIF([.95, .9])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We move on to define the loss function of our model. This is particularly easy and one of the many instances where `JAX` really shines.\n",
    "As opposed to other frameworks, we can define our loss function for a single sample only and then use the `jax.vmap` function transformation to automatically batchify this function. Use the `None` keyword for the arguments of your function that you do not want to batchify. Learn more about the awesome features of `JAX` under [JAX Introduction](https://jax.readthedocs.io/en/latest/quickstart.html#auto-vectorization-with-jax-vmap).\n",
    "\n",
    "It is time to have a quick talk about the intricacies of `JAX` and `equinox` when it comes to stateful computations and the management of parameters.\n",
    "\n",
    "\n",
    "Here we define how the loss is exacly calculated, i.e. whether\n",
    "we use a sum of spikes or spike-timing for the calculation of\n",
    "the cross-entropy. For a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.vmap(in_axes=(None, None, 0, 0, 0))\n",
    "def loss_fn(model, init_states, data, target, key):\n",
    "    states, outs = model(init_states, data, key=key)\n",
    "\n",
    "    # Get the output of last layer\n",
    "    final_layer_out = outs[-1]\n",
    "    # Sum all spikes in each output neuron along time axis\n",
    "    pred = tree_map(lambda x: jnp.sum(x, axis=0), final_layer_out)\n",
    "    return optax.softmax_cross_entropy(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the function to compute the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_value_and_grad\n",
    "def loss_and_grads(model, init_states, data, target, key):\n",
    "    return jnp.sum(loss_fn(model, init_states, data, target, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the update function\n",
    "Function to calculate the update of the model and the optimizer based\n",
    "on the calculated updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def update(model,\n",
    "            optim, \n",
    "            opt_state, \n",
    "            input_batch, \n",
    "            target_batch, \n",
    "            loss_fn, \n",
    "            key):\n",
    "    \"\"\"\n",
    "    Function to calculate the update of the model and the optimizer based\n",
    "    on the calculated updates.\n",
    "    \"\"\"\n",
    "    init_key, grad_key = jax.random.split(key)\n",
    "    states = model.init_state(SENSOR_SIZE, init_key)\n",
    "    loss_value, grads = loss_and_grads(model, \n",
    "                                        states, \n",
    "                                        input_batch, \n",
    "                                        target_batch, \n",
    "                                        loss_fn, \n",
    "                                        grad_key)    \n",
    "\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "\n",
    "optim = optax.adam(LR)\n",
    "opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "nbar = tqdm(range(EPOCHS))\n",
    "\n",
    "for epoch in nbar:\n",
    "    losses = []\n",
    "    \n",
    "    pbar = tqdm(train_dataloader, leave=False)\n",
    "    for input_batch, target_batch in pbar:\n",
    "        model_key, batch_key, key = jrand.split(key, 3)\n",
    "        input_batch = jnp.asarray(input_batch.numpy(), dtype=jnp.float32)\n",
    "        target_batch = jnp.asarray(target_batch.numpy(), dtype=jnp.float32)\n",
    "        one_hot_target_batch = jnp.asarray(nn.one_hot(target_batch, NUM_LABELS), \n",
    "                                            dtype=jnp.float32)\n",
    "\n",
    "        model, opt_state, loss = update(model, \n",
    "                                        optim,\n",
    "                                        opt_state,  \n",
    "                                        input_batch,\n",
    "                                        one_hot_target_batch,\n",
    "                                        model_key\n",
    "                                    )\n",
    "            \n",
    "        losses.append(loss/BATCHSIZE)\n",
    "        \n",
    "        pbar.set_description(f\"loss: {loss/BATCHSIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbar = tqdm(test_dataloader, leave=False)  \n",
    "test_accuracies = []\n",
    "for input_test, target_test in tbar:\n",
    "    batch_key, key = jrand.split(key, 2)\n",
    "    input_batch = jnp.asarray(input_test.numpy(), dtype=jnp.float32)\n",
    "    target_batch = jnp.asarray(target_test.numpy(), dtype=jnp.float32)\n",
    "    test_acc = calc_accuracy(model, \n",
    "                            model.init_state(SENSOR_SIZE, batch_key), \n",
    "                            input_batch, \n",
    "                            target_batch,\n",
    "                            key)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "model = eqx.tree_inference(model, False)\n",
    "\n",
    "nbar.set_description(f\"epoch: {epoch}, \"\n",
    "                    f\"loss = {jnp.mean(losses)}, \"\n",
    "                    f\"test_accuracy = {jnp.mean(test_accuracies):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snnax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
